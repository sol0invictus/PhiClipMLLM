model:
  text_model: microsoft/Phi-3-mini-4k-instruct
  vision_model: openai/clip-vit-large-patch14
  adapter_dim: 1024

dtype: torch.bfloat16

checkpoint:
  output_dir: C:\Users\sunny\Desktop\mllm\checkpoint_dir\
  load_dir: C:\Users\sunny\Desktop\mllm\checkpoint_dir\checkpoint-14786\model_checkpoint.pt

phase_one:
  annotation_file: C:/coco/labels/annotations/captions_train2017.json
  image_directory: C:/coco/train2017

phase_two:
  directory: C:\Users\sunny\Desktop\mllm\datasets\phase_two

training_args:
  num_train_epochs: 1
  learning_rate: 0.0001
  batch_size: 4
  save_steps: 500

peft_text:
  r: 64
  lora_alpha: 64
  target_modules: ["qkv_proj", "o_proj"]
  lora_dropout: 0.1
  bias: none

peft_vision:
  r: 64
  lora_alpha: 64
  target_modules: ["q_proj", "v_proj", "o_proj"]
  lora_dropout: 0.1
  bias: none

peft_adapter:
  r: 12
  lora_alpha: 2
  target_modules: ["0", "2"]
  lora_dropout: 0.1
  bias: none